{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo2-test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Lj1b6Id1oX"
      },
      "source": [
        "# Backprop by hand\n",
        "\n",
        "This is a short demo of coding up backpropagation from scratch. \n",
        "\n",
        "*Health warning*: Don't really do this for each model. Just rely on Pytorch's\n",
        "\n",
        "```\n",
        " backward()\n",
        "```\n",
        "instead.\n",
        "\n",
        "First, let's download our usual MNIST dataset. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kjvPSnDA4J_w",
        "outputId": "4cb0a15e-b23d-4881-de61-83d5a938ecce"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "\n",
        "plt.imshow(x_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1ca2241210>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klx9qPmxF9jI"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv-CMWayfRyP"
      },
      "source": [
        "It will be helpful to recall the following definitions:\n",
        "\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "$\\nabla \\sigma(x) = \\sigma(x)(1 - \\sigma(x))$\n",
        "\n",
        "$\\text{softmax}(z)|_i = \\frac{e^{z_i}}{\\sum e^{z_i}}$\n",
        "\n",
        "$CE(y,\\hat{y}) = - \\sum y_i \\log \\hat{y}_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdyvaUKoF7ux"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  x = np.clip(x, -500, 500)\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def dsigmoid(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "  b = x.max()\n",
        "  y = np.exp(x - b)\n",
        "  return y / y.sum()\n",
        "\n",
        "def cross_entropy_loss(y, yHat):\n",
        "  return -np.sum(y * np.log(yHat))\n",
        "\n",
        "def integer_to_one_hot(x, numclasses):\n",
        "  result = np.zeros(numclasses)\n",
        "  result[x] = 1\n",
        "  return result"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIZEupTHyNM"
      },
      "source": [
        "# Initialize architecture, weights, and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBeGvbu6FaM_"
      },
      "source": [
        "import math\n",
        "\n",
        "weights = [\n",
        "  np.random.normal(0, 1/math.sqrt(784), (784, 100)),\n",
        "  np.random.normal(0, 1/math.sqrt(100), (100, 10))\n",
        "]\n",
        "biases = [np.zeros((1,100)), np.zeros((1,10))]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP3nT3u-8Hsc"
      },
      "source": [
        "Let's just check to see that our network works by setting up a forward pass function. (This will also help evaluate test error later on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSs7_MKRt7TV"
      },
      "source": [
        "def feed_forward_sample(sample, y):\n",
        "  a = np.reshape(sample,(1,28*28))\n",
        "\n",
        "  # Forward pass\n",
        "  u1 = np.dot(a,weights[0]) + biases[0]\n",
        "  z1 = sigmoid(u1)\n",
        "  u2 = np.dot(z1,weights[1]) + biases[1]\n",
        "  yhat = softmax(u2)\n",
        "\n",
        "  # Calculate loss\n",
        "  pred_class = np.argmax(yhat)\n",
        "  one_hot_guess = integer_to_one_hot(pred_class,10)\n",
        "  yvector = integer_to_one_hot(y,10)\n",
        "  loss = cross_entropy_loss(yvector,yhat)\n",
        "  \n",
        "  return loss, one_hot_guess\n",
        "\n",
        "def feed_forward_dataset(x, y):\n",
        "  losses = np.empty(x.shape[0])\n",
        "  one_hot_guesses = np.empty((x.shape[0], 10))\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    sample = np.reshape(x[i],(1,28*28))\n",
        "    losses[i], one_hot_guesses[i] = feed_forward_sample(sample, y[i])\n",
        "\n",
        "  y_one_hot = np.zeros((y.size, 10))\n",
        "  y_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "  total_correct_guesses = np.sum(y_one_hot * one_hot_guesses)\n",
        "  accuracy = format((total_correct_guesses / y.shape[0]) * 100, \".2f\")\n",
        "  print(\"Accuracy:\", accuracy, \"%)\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrlc2VLOi8L"
      },
      "source": [
        "# Backprop\n",
        "\n",
        "We go back up the computation graph, starting with differentiating the loss function with respect to the last-layer activations, and successively go backwards using the chain rule. Recall that the computation graph looks like:\n",
        "\n",
        "$x \\rightarrow u_1 \\rightarrow z_1 \\rightarrow u_2 \\rightarrow \\hat{y} \\rightarrow L$\n",
        "\n",
        "Some equations to consider:\n",
        "\n",
        "$\\partial_{u_2} L = \\hat{y} - y$\n",
        "\n",
        "$\\partial_{w_2} L = \\partial_{u_2} L \\cdot \\partial_{w_2} u_2 = (\\hat{y} - y) z_1^T$\n",
        "\n",
        "$\\partial_{b_2} L = \\partial_{u_2} L \\cdot \\partial_{b_2} u_2 = \\hat{y} - y$\n",
        "\n",
        "$\\partial_{z_1} L = \\partial_{u_2} L \\cdot \\partial_{z_1} u_2 = (\\hat{y} - y) \\cdot W_1$\n",
        "\n",
        "$\\partial_{u_1} L = \\partial_{z_1} L \\cdot \\partial_{u_1} z_1 = (\\hat{y} - y) \\cdot W_1 \\cdot \\sigma'(u_1)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLgrl6VNn8pB"
      },
      "source": [
        "def train_one_sample(sample, y, learning_rate=0.003):\n",
        "  a = np.reshape(sample,(1,28*28))\n",
        "\n",
        "  # intermediate variables\n",
        "  weight_gradients = []\n",
        "  bias_gradients = []\n",
        "\n",
        "  # Forward pass\n",
        "  u1 = np.dot(a,weights[0]) + biases[0]\n",
        "  z1 = sigmoid(u1)\n",
        "  u2 = np.dot(z1,weights[1]) + biases[1]\n",
        "  yhat = softmax(u2)\n",
        "\n",
        "  # Calculate loss\n",
        "  pred_class = np.argmax(yhat)\n",
        "  one_hot_guess = integer_to_one_hot(pred_class,10)\n",
        "  yvector = integer_to_one_hot(y,10)\n",
        "  loss = cross_entropy_loss(yvector,yhat)\n",
        "\n",
        "  # Backward pass\n",
        "  du2 = yhat - yvector\n",
        "  dw2 = z1.T.dot(du2)\n",
        "  db2 = du2\n",
        "  \n",
        "  dz1 = du2.dot(weights[1].T)\n",
        "  du1 = np.multiply(dz1,dsigmoid(u1))\n",
        "  dw1 = a.T.dot(du1)\n",
        "  db1 = du1\n",
        "\n",
        "  weight_gradients.append(dw1)\n",
        "  weight_gradients.append(dw2)\n",
        "  bias_gradients.append(db1)\n",
        "  bias_gradients.append(db2)\n",
        "\n",
        "  # update weights\n",
        "  num_layers = 2\n",
        "  for i in range(num_layers):\n",
        "    weights[i] = weights[i] - learning_rate * weight_gradients[i]\n",
        "    biases[i] = biases[i] - learning_rate * bias_gradients[i]\n",
        "  \n",
        "  return loss\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNMxpc7WLs6u"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Great! Let's see how well we do. We can run the backward pass sample-by-sample, collect (all) gradients, and use them to update our weights/biases. Normally we would do this on the GPU; using plain numpy and CPU will take some time so we only train for 3 epochs.\n",
        "\n",
        "After each epoch, we measure and report the test accuracy. We see that within a couple of epochs we reach close to 90\\% accuracy, which is about the best we can hope to do with a one-hidden-layer network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLLEsVdcOgzi",
        "outputId": "331cab48-cca9-4b6a-dfed-55e2dc33e19a"
      },
      "source": [
        "def train_one_epoch(learning_rate=0.001):\n",
        "  print(\"Training for one epoch over the training dataset...\")\n",
        "  for i in range(x_train.shape[0]):\n",
        "    train_one_sample(x_train[i], y_train[i], learning_rate)\n",
        "  print(\"Finished training.\\n\")\n",
        "\n",
        "# Train and check accuracy before & after each epoch\n",
        "\n",
        "def test_and_train():\n",
        "  train_one_epoch()\n",
        "  feed_forward_dataset(x_test, y_test)\n",
        "\n",
        "for i in range(3): # Adjust number of epochs here\n",
        "  test_and_train()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for one epoch over the training dataset...\n",
            "Finished training.\n",
            "\n",
            "Accuracy: 87.45 %)\n",
            "Training for one epoch over the training dataset...\n",
            "Finished training.\n",
            "\n",
            "Accuracy: 87.09 %)\n",
            "Training for one epoch over the training dataset...\n",
            "Finished training.\n",
            "\n",
            "Accuracy: 87.20 %)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VILEun5C7o-0"
      },
      "source": [
        "Great! Feel free to play around with architectures, learning rates, etc to figure out if you can get better performance. (Also, the datasets are unnormalized; it may help to pre-process all data points and normalize to [0,1]."
      ]
    }
  ]
}