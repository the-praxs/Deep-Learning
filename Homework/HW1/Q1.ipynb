{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q1 Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up device as CUDA if available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Net:\n",
    "  def __init__(self, x):\n",
    "    # Input\n",
    "    self.x = x\n",
    "\n",
    "    # Initializing first set of weights and bias as Torch tensors\n",
    "    self.Wh = torch.tensor([[2, 4], [3, -5]], dtype=torch.int32)\n",
    "    self.bh = torch.tensor([[-2], [1]], dtype=torch.int32)\n",
    "\n",
    "    # Initializing second set of weights and bias as Torch tensors\n",
    "    self.Wo = torch.tensor([4, 6], dtype=torch.int32)\n",
    "    self.bo = torch.tensor([-3.5], dtype=torch.float32)\n",
    "\n",
    "    # Initializing all computations as 0\n",
    "    self.zh = 0\n",
    "    self.uh = 0\n",
    "    self.zo = 0\n",
    "    self.y = 1\n",
    "      \n",
    "  def sigmoid(self, x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "  def ReLU(self, x):\n",
    "    return torch.maximum(torch.zeros_like(x), x)\n",
    "    \n",
    "  def cross_entropy_loss(self, yHat, y):\n",
    "    return -torch.sum(y * torch.log(yHat))\n",
    "  \n",
    "  def forward_pass(self):\n",
    "    # Linear Map 1\n",
    "    self.zh = torch.matmul(self.Wh, self.x) + self.bh\n",
    "    \n",
    "    # Activation ReLU\n",
    "    self.uh = self.ReLU(self.zh)\n",
    "\n",
    "    # Linear Map 2\n",
    "    self.zo = torch.matmul(self.Wo, self.uh) + self.bo\n",
    "    # Activation Sigmoid\n",
    "    yHat = self.sigmoid(self.zo)\n",
    "\n",
    "    # Calculate Cross-Entropy Loss\n",
    "    loss = self.cross_entropy_loss(yHat, self.y)\n",
    "\n",
    "    return loss, yHat\n",
    "\n",
    "  def backward_pass(self, yHat):\n",
    "    dLoss = -(self.y/yHat) + ((1 - self.y) / (1 - yHat))\n",
    "    dyHat = self.sigmoid(self.zo) * (1 - self.sigmoid(self.zo))\n",
    "    dzo_Wo = self.uh\n",
    "    dzo_bo = 1\n",
    "    dzo_uh = self.Wo\n",
    "    duh = torch.tensor([[1 if self.zh[0]>=0 else 0], [1 if self.zh[1]>=0 else 0]], dtype=torch.int32)\n",
    "    dzh_Wh = self.x\n",
    "    dzh_bh = 1\n",
    "\n",
    "    # Calculating the gradients\n",
    "    grad_L_Wo = dLoss * dyHat * dzo_Wo\n",
    "    grad_L_bo = dLoss * dyHat * dzo_bo\n",
    "    grad_L_Wh = dLoss * dyHat * dzo_uh * duh * dzh_Wh\n",
    "    grad_L_bh = dLoss * dyHat * dzo_uh * duh * dzh_bh\n",
    "\n",
    "    return grad_L_Wo, grad_L_bo, grad_L_Wh, grad_L_bh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing the Forward phase and Backward phase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "x = torch.tensor([[1], [1]], dtype=torch.int32)\n",
    "\n",
    "# Forward phase\n",
    "n = Net(x)\n",
    "loss, yHat = n.forward_pass()\n",
    "\n",
    "# Backward phase\n",
    "grad_L_Wo, grad_L_bo, grad_L_Wh, grad_L_bh = n.backward_pass(yHat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction =  tensor([1.0000])\n",
      "Loss =  tensor(3.6955e-06)\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction = \", yHat)\n",
    "print(\"Loss = \", loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dWo =  tensor([[-1.4782e-05],\n",
      "        [-0.0000e+00]]) \n",
      "\n",
      "dL/dbo =  tensor([-3.6955e-06]) \n",
      "\n",
      "dL/dWh =  tensor([[-1.4782e-05, -2.2173e-05],\n",
      "        [-0.0000e+00, -0.0000e+00]]) \n",
      "\n",
      "dL/dbh =  tensor([[-1.4782e-05, -2.2173e-05],\n",
      "        [-0.0000e+00, -0.0000e+00]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"dL/dWo = \", grad_L_Wo, '\\n')\n",
    "print(\"dL/dbo = \", grad_L_bo, '\\n')\n",
    "print(\"dL/dWh = \", grad_L_Wh, '\\n')\n",
    "print(\"dL/dbh = \", grad_L_bh, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End of Notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}